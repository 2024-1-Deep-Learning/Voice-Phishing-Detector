# -*- coding: utf-8 -*-
"""text_augmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMiqruRwV22Jil7zEExz34C1dv11bqDo
"""

import random
import pandas as pd
import os
import pickle
import re

from google.colab import drive
drive.mount('/content/drive')

file_path="/content/drive/MyDrive/deep-learning/team_project/wordnet.pickle"
word_path="/content/drive/MyDrive/deep-learning/team_project/voice_data/split_voicephishing.csv"

data=pd.read_csv(word_path)
sentences=data['sentence'].tolist()

import os

if not os.path.exists(file_path):
    print(f"File {file_path} not found. Please ensure the file path is correct.")
else:
    # Load the pickle file
    with open(file_path, "rb") as f:
        wordnet = pickle.load(f)
    print("File loaded successfully.")
    # Now you can use the 'wordnet' dictionary as needed

def get_only_hangul(line):
	parseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',line)

	return parseText

print(f"Type of wordnet: {type(wordnet)}")

# Print the keys of the wordnet dictionary (if it is a dictionary)
if isinstance(wordnet, dict):
    print(f"Number of words in wordnet: {len(wordnet)}")
    sample_keys = list(wordnet.keys())[:10]
    print(f"Sample keys: {sample_keys}")

    # Print some sample data for the first few words
    for key in sample_keys:
        print(f"Word: {key}, Synonyms: {wordnet[key]}")

#synonym replacement
#replace n words in the sentence with synonyms from wordnet
def synonym_replacement(words, n):
	new_words = words.copy()
	random_word_list = list(set([word for word in words]))
	random.shuffle(random_word_list)
	num_replaced = 0
	for random_word in random_word_list:
		synonyms = get_synonyms(random_word)
		if len(synonyms) >= 1:
			synonym = random.choice(list(synonyms))
			new_words = [synonym if word == random_word else word for word in new_words]
			num_replaced += 1
		if num_replaced >= n:
			break

	if len(new_words) != 0:
		sentence = ' '.join(new_words)
		new_words = sentence.split(" ")

	else:
		new_words = ""

	return new_words


def get_synonyms(word):
	synomyms = []

	try:
		for syn in wordnet[word]:
			for s in syn:
				synomyms.append(s)
	except:
		pass

	return synomyms



	augmented_sentences=[]
n=10

for sentence in sentences:
    words = sentence.split()  # Segment the sentence into words
    augmented_sentence = synonym_replacement(words, n)
    augmented_sentences.append(' '.join(augmented_sentence))

data['augmented_sentence'] = augmented_sentences
augmented_csv_file_path = '/content/drive/MyDrive/deep-learning/team_project/voice_data/Synonym_Replacement/SR_10.csv'  # Update this path if needed
data.to_csv(augmented_csv_file_path, index=False)

#Random Deletion


def random_deletion(words, p):
	if len(words) == 1:
		return words

	new_words = []
	for word in words:
		r = random.uniform(0, 1)
		if r > p:
			new_words.append(word)

	if len(new_words) == 0:
		rand_int = random.randint(0, len(words)-1)
		return [words[rand_int]]

	return new_words


	deletion_probability=0.6

augmented_sentences = []

for sentence in sentences:
    words = sentence.split()  # Segment the sentence into words
    augmented_words = random_deletion(words, deletion_probability)
    augmented_sentence = ' '.join(augmented_words)
    augmented_sentences.append(augmented_sentence)

data['augmented_sentence'] = augmented_sentences

augmented_csv_file_path = '/content/drive/MyDrive/deep-learning/team_project/voice_data/Random_deletion/RD_0.6.csv'  # Update this path if needed
data.to_csv(augmented_csv_file_path, index=False)

print(f"Augmented sentences saved to {augmented_csv_file_path}")

#Random swap

def random_swap(words, n):
	new_words = words.copy()
	for _ in range(n):
		new_words = swap_word(new_words)

	return new_words

def swap_word(new_words):
	random_idx_1 = random.randint(0, len(new_words)-1)
	random_idx_2 = random_idx_1
	counter = 0

	while random_idx_2 == random_idx_1:
		random_idx_2 = random.randint(0, len(new_words)-1)
		counter += 1
		if counter > 3:
			return new_words

	new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
	return new_words

	num_swaps= 2
augmented_sentences= []

for sentence in sentences:
  words = sentence.split()  # Segment the sentence into words
  augmented_words = random_swap(words, num_swaps)
  augmented_sentence = ' '.join(augmented_words)
  augmented_sentences.append(augmented_sentence)

data['augmented_sentence'] = augmented_sentences
augmented_csv_file_path = '/content/drive/MyDrive/deep-learning/team_project/voice_data/RD_9.csv'  # Update this path if needed
data.to_csv(augmented_csv_file_path, index=False)

#Random Insertion
#Randomly insert n words into the sentence

def random_insertion(words, n):
	new_words = words.copy()
	for _ in range(n):
		add_word(new_words)

	return new_words


def add_word(new_words):
	synonyms = []
	counter = 0
	while len(synonyms) < 1:
		if len(new_words) >= 1:
			random_word = new_words[random.randint(0, len(new_words)-1)]
			synonyms = get_synonyms(random_word)
			counter += 1
		else:
			random_word = ""

		if counter >= 10:
			return

	random_synonym = synonyms[0]
	random_idx = random.randint(0, len(new_words)-1)
	new_words.insert(random_idx, random_synonym)



def EDA(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):
	sentence = get_only_hangul(sentence)
	words = sentence.split(' ')
	words = [word for word in words if word is not ""]
	num_words = len(words)

	augmented_sentences = []
	num_new_per_technique = int(num_aug/4) + 1

	n_sr = max(1, int(alpha_sr*num_words))
	n_ri = max(1, int(alpha_ri*num_words))
	n_rs = max(1, int(alpha_rs*num_words))

	# sr
	for _ in range(num_new_per_technique):
		a_words = synonym_replacement(words, n_sr)
		augmented_sentences.append(' '.join(a_words))

	# ri
	for _ in range(num_new_per_technique):
		a_words = random_insertion(words, n_ri)
		augmented_sentences.append(' '.join(a_words))

	# rs
	for _ in range(num_new_per_technique):
		a_words = random_swap(words, n_rs)
		augmented_sentences.append(" ".join(a_words))

	# rd
	for _ in range(num_new_per_technique):
		a_words = random_deletion(words, p_rd)
		augmented_sentences.append(" ".join(a_words))

	augmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]
	random.shuffle(augmented_sentences)

	if num_aug >= 1:
		augmented_sentences = augmented_sentences[:num_aug]
	else:
		keep_prob = num_aug / len(augmented_sentences)
		augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]

	augmented_sentences.append(sentence)

	return augmented_sentences

