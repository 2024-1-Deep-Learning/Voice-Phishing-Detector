# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B032QFdBKrsnoYDm1uUghorFa_w_D7a6
"""

pip install imblearn

from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout,BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model
from keras.callbacks import Callback
from keras.layers import Attention, GlobalMaxPooling1D
from google.colab import drive
from keras.models import Sequential
from gensim.models import Word2Vec
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import RandomUnderSampler
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from gensim.models import Word2Vec

from google.colab import drive
drive.mount('/content/drive')

phishing="/content/drive/My Drive/deep_learning/agumented_voicephishing.csv"
Non_phishing="/content/drive/My Drive/deep_learning/combined_sentences_nochr.csv"

phishing_df=pd.read_csv(phishing)
non_phishing_df=pd.read_csv(Non_phishing)
non_phishing_df.rename(columns={'어, 네, 될 거 같아요.':'sentence'},inplace=True)
phishing_df['label']=1
non_phishing_df['label']=0

print(len(phishing_df))
print(len(non_phishing_df))

print(phishing_df.tail())

print(non_phishing_df.head())

#combine data of both phishing & Non-phishing data
combined_df = pd.concat([phishing_df, non_phishing_df], ignore_index=True)
combined_df.dropna(subset=['sentence'],inplace=True )
X = combined_df.drop(columns=['label'])
y = combined_df['label']

# Filter out rows with sentences containing only one character
combined_df = combined_df[combined_df['sentence'].str.len() > 10]

print(len(combined_df))

# Calculate the number of phishing samples
phishing_count = y.value_counts()[1]

# Desired number of non-phishing samples (2 times phishing samples for a 1:2 ratio)
desired_non_phishing_count = phishing_count * 3

# Calculate the sampling strategy
sampling_strategy = {0: desired_non_phishing_count, 1: phishing_count}

# Random undersampling
rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)
X_resampled, y_resampled = rus.fit_resample(X, y)

# Combine resampled features and target
resampled_df = pd.concat([X_resampled, y_resampled], axis=1)

print(len(resampled_df))

print(resampled_df['label'].value_counts())

print(resampled_df.tail())

#tokenization
sentences = resampled_df['sentence'].tolist()
labels = resampled_df['label'].tolist()

tokenizer= Tokenizer()
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
max_length = max(len(sequences) for sequence in sequences)

print(len(sequences))
print(len(labels))

sequence_lengths = [len(seq) for seq in sequences]
print("Min sequence length:", min(sequence_lengths))
print("Max sequence length:", max(sequence_lengths))
print("Average sequence length:", np.mean(sequence_lengths))

#padding sequence
padded_sequences = pad_sequences(sequences, maxlen=80)

print(padded_sequences)

#embedding sequnece using Pretrained Word2Vec Model
word2vec_model = Word2Vec(sentences=sequences, vector_size=100, window=5, min_count=1, workers=4)
embedding_matrix=word2vec_model.wv.vectors

embedding_matrix = np.vstack([np.zeros((1, embedding_matrix.shape[1])), embedding_matrix])

print(embedding_matrix.shape)

if np.max(padded_sequences) >= embedding_matrix.shape[0]:
    print("Error: Index out of range")
    print("Maximum index in padded sequences:", np.max(padded_sequences))

max_index_padded_sequences = np.max(padded_sequences)

max_index_embedding_matrix = embedding_matrix.shape[0] - 1  # Adjust for zero-based indexing

if max_index_padded_sequences >= max_index_embedding_matrix:
    print("Error: Index out of range")
    print("Maximum index in padded sequences:", max_index_padded_sequences)
    print("Maximum index in embedding matrix:", max_index_embedding_matrix)

embedded_sequences = np.array([embedding_matrix[indices] for indices in padded_sequences])

embedded_sequences.shape

def batch_process_embedding(sequences, embedding_matrix, batch_size=10000, output_dir='embedded_batches'):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    num_sequences = sequences.shape[0]
    for start in range(0, num_sequences, batch_size):
        end = min(start + batch_size, num_sequences)
        batch_sequences = sequences[start:end]
        embedded_batch = np.array([embedding_matrix[indices] for indices in batch_sequences])
        batch_filename = os.path.join(output_dir, f'batch_{start // batch_size}.npy')
        np.save(batch_filename, embedded_batch)
        print(f'Saved {batch_filename}')



batch_process_embedding(padded_sequences, embedding_matrix)

# Function to load and concatenate all batches
def load_and_concatenate_batches(output_dir='embedded_batches'):
    batch_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.npy')]
    batch_files.sort()  # Ensure the batches are loaded in order
    concatenated_data = []
    for batch_file in batch_files:
        batch_data = np.load(batch_file)
        concatenated_data.append(batch_data)
    return np.concatenate(concatenated_data, axis=0)

embedded_sequences = load_and_concatenate_batches()
print(embedded_sequences.shape)

embedded_sequences = load_and_concatenate_batches()

# Save the concatenated embedded sequences to disk
np.save('embedded_sequences.npy', embedded_sequences)

file_path = 'embedded_sequences.npy'

# Check if the file exists
if os.path.exists(file_path):
    # Delete the file
    os.remove(file_path)
    print(f"File '{file_path}' deleted successfully.")
else:
    print(f"File '{file_path}' does not exist.")

#shape of embedded sequence of whole data
print(embedded_sequences.shape)

len(labels)
print(len(embedded_sequences))

# Convert labels to numpy array
labels = np.array(labels)

# Check if the lengths match
if len(labels) != len(embedded_sequences):
    print("Error: Length mismatch between labels and embedded sequences")
else:
    # Shuffle indices
    shuffled_indices = np.random.permutation(len(embedded_sequences))

    # Use shuffled indices to shuffle embedded sequences and labels
    embedded_sequences_shuffled = embedded_sequences[shuffled_indices]
    labels_shuffled = labels[shuffled_indices]

    # Split data into training and temporary sets (train_temp and test) with stratification
    X_train_temp, X_test, y_train_temp, y_test = train_test_split(
        embedded_sequences_shuffled, labels_shuffled, test_size=0.2, stratify=labels_shuffled, random_state=42
    )

    # Further split the train_temp into training and validation sets with stratification
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_temp, y_train_temp, test_size=0.25, stratify=y_train_temp, random_state=42
    )

#check the distribution of train,test,validation data's proportion

import numpy as np

# Function to print the class distribution
def print_class_distribution(y, dataset_name):
    unique, counts = np.unique(y, return_counts=True)
    distribution = dict(zip(unique, counts))
    print(f"Class distribution in {dataset_name}:")
    for label, count in distribution.items():
        print(f"Class {label}: {count} samples, {count / len(y) * 100:.2f}%")

# Print class distributions
print_class_distribution(y_train, "Training set")
print_class_distribution(y_val, "Validation set")
print_class_distribution(y_test, "Test set")

# Define custom attention layer
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1), initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = x * a
        return K.sum(output, axis=1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])

# Define model architecture with enhanced layers
input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))
conv_layer = Conv1D(128, 5, activation='relu')(input_layer)
conv_layer = BatchNormalization()(conv_layer)
dropout_layer = Dropout(0.5)(conv_layer)
bidirectional_lstm = Bidirectional(LSTM(64, return_sequences=True))(dropout_layer)
attention_layer = AttentionLayer()(bidirectional_lstm)
global_max_pooling = GlobalMaxPooling1D()(bidirectional_lstm)
dense_layer = Dense(64, activation='relu')(global_max_pooling)
dense_layer = BatchNormalization()(dense_layer)
dropout_layer_2 = Dropout(0.5)(dense_layer)
output_layer = Dense(1, activation='sigmoid')(dropout_layer_2)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile model with additional metrics
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])

model.summary()

# Train model with early stopping
from keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping, checkpoint])

# Print epoch number for every iteration
for epoch in range(1, len(history.history['loss']) + 1):
    print(f'Epoch {epoch}/{len(history.history["loss"])}')

# Evaluate model on the test set
loss, accuracy = model.evaluate(X_test, y_test)

# Print out the evaluation metrics
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy*100:.2f}%")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt


y_val_pred = model.predict(X_val)
# Since this is a binary classification, convert probabilities to class labels
y_val_pred = (y_val_pred > 0.5).astype("int32")

# Calculate evaluation metrics
accuracy = accuracy_score(y_val, y_val_pred)
precision = precision_score(y_val, y_val_pred)
recall = recall_score(y_val, y_val_pred)
f1 = f1_score(y_val, y_val_pred)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

from keras.utils import to_categorical

# Convert labels to one-hot encoding
labels_one_hot = to_categorical(labels, num_classes=2)

# Define the model
model = Sequential()

vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token
embedding_dim = 100  # Assuming you have 100-dimensional embeddings
max_length = 80  # Maximum sequence length after padding
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))

# Add convolutional layer
model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(Bidirectional(LSTM(units=64, return_sequences=True)))
# Add output layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print model summary
model.summary()

class PrintEpoch(Callback):
    def on_epoch_begin(self, epoch, logs=None):
        print(f"Epoch {epoch + 1}/{self.params['epochs']}")

# Create an instance of the PrintEpoch callback
print_epoch = PrintEpoch()