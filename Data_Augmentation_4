Korean_Data_Augmentation
#4 viable data augmentation method applied for korean speech-to-text data
#

import random
import pandas as pd
import os
import pickle
import re

from google.colab import drive
drive.mount('/content/drive')


data=pd.read_csv(word_path)
sentences=data['sentence'].tolist()

import os

if not os.path.exists(file_path):
    print(f"File {file_path} not found. Please ensure the file path is correct.")
else:
    # Load the pickle file
    with open(file_path, "rb") as f:
        wordnet = pickle.load(f)
    print("File loaded successfully.")
    # Now you can use the 'wordnet' dictionary as needed

def get_only_hangul(line):
	parseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',line)

	return parseText













#synonym replacement
#replace n words in the sentence with synonyms from wordnet
def synonym_replacement(words, n):
	new_words = words.copy()
	random_word_list = list(set([word for word in words]))
	random.shuffle(random_word_list)
	num_replaced = 0
	for random_word in random_word_list:
		synonyms = get_synonyms(random_word)
		if len(synonyms) >= 1:
			synonym = random.choice(list(synonyms))
			new_words = [synonym if word == random_word else word for word in new_words]
			num_replaced += 1
		if num_replaced >= n:
			break

	if len(new_words) != 0:
		sentence = ' '.join(new_words)
		new_words = sentence.split(" ")

	else:
		new_words = ""

	return new_words


def get_synonyms(word):
	synomyms = []

	try:
		for syn in wordnet[word]:
			for s in syn:
				synomyms.append(s)
	except:
		pass

	return synomyms



augmented_sentences=[]
n=10
for sentence in sentences:
    words = sentence.split()  # Segment the sentence into words
    augmented_sentence = synonym_replacement(words, n)
    augmented_sentences.append(' '.join(augmented_sentence))

data['augmented_sentence'] = augmented_sentences
augmented_csv_file_path = '/content/drive/MyDrive/deep-learning/team_project/voice_data/Synonym_Replacement/SR_10.csv'  # Update this path if needed
data.to_csv(augmented_csv_file_path, index=False)














#Random Deletion


def random_deletion(words, p):
	if len(words) == 1:
		return words

	new_words = []
	for word in words:
		r = random.uniform(0, 1)
		if r > p:
			new_words.append(word)

	if len(new_words) == 0:
		rand_int = random.randint(0, len(words)-1)
		return [words[rand_int]]

	return new_words


deletion_probability=0.6

augmented_sentences = []

for sentence in sentences:
    words = sentence.split()  # Segment the sentence into words
    augmented_words = random_deletion(words, deletion_probability)
    augmented_sentence = ' '.join(augmented_words)
    augmented_sentences.append(augmented_sentence)

data['augmented_sentence'] = augmented_sentences

augmented_csv_file_path = '/content/drive/MyDrive/deep-learning/team_project/voice_data/Random_deletion/RD_0.6.csv'  # Update this path if needed
data.to_csv(augmented_csv_file_path, index=False)

print(f"Augmented sentences saved to {augmented_csv_file_path}")












#Random swap

def random_swap(words, n):
	new_words = words.copy()
	for _ in range(n):
		new_words = swap_word(new_words)

	return new_words

def swap_word(new_words):
	random_idx_1 = random.randint(0, len(new_words)-1)
	random_idx_2 = random_idx_1
	counter = 0

	while random_idx_2 == random_idx_1:
		random_idx_2 = random.randint(0, len(new_words)-1)
		counter += 1
		if counter > 3:
			return new_words

	new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
	return new_words

num_swaps= 2
augmented_sentences= []

for sentence in sentences:
  words = sentence.split()  # Segment the sentence into words
  augmented_words = random_swap(words, num_swaps)
  augmented_sentence = ' '.join(augmented_words)
  augmented_sentences.append(augmented_sentence)

data['augmented_sentence'] = augmented_sentences
augmented_csv_file_path = '/content/drive/MyDrive/deep-learning/team_project/voice_data/RD_9.csv'  # Update this path if needed
data.to_csv(augmented_csv_file_path, index=False)

















#Random Insertion(RI)
def get_synonyms(word):
    synonyms = []
    try:
        for syn in wordnet[word]:
            for s in syn:
                synonyms.append(s)
    except KeyError:
        pass
    return synonyms

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        if len(new_words) >= 1:
            random_word = new_words[random.randint(0, len(new_words)-1)]
            synonyms = get_synonyms(random_word)
            counter += 1
        else:
            random_word = ""
        if counter >= 10:
            return
    if synonyms:  # Ensure there is at least one synonym
        random_synonym = synonyms[0]
        random_idx = random.randint(0, len(new_words)-1)
        new_words.insert(random_idx, random_synonym)

def get_only_hangul(sentence):
    return ''.join([char for char in sentence if re.match(r'[ㄱ-ㅎ|ㅏ-ㅣ|가-힣]', char)])


# Assuming the sentences are in a column named 'sentence'
sentences = data['sentence'].tolist()

# Define the number of insertions
num_insertions = 9  # Adjust as needed

augmented_data = []

for sentence in sentences:
    words = sentence.split()  # Segment the sentence into words
    augmented_words = random_insertion(words, num_insertions)
    augmented_sentence = ' '.join(augmented_words)
    augmented_data.append({'original_sentence': sentence, 'augmented_sentence': augmented_sentence})

# Create a DataFrame for augmented data
augmented_df = pd.DataFrame(augmented_data)

# Save the augmented data to a new CSV file
augmented_csv_file_path = '/content/drive/My Drive/deep-learning/team_project/voice_data/Random_Insertion/RI_9.csv'  # Update this path if needed
augmented_df.to_csv(augmented_csv_file_path, index=False)

print(f"Augmented sentences saved to {augmented_csv_file_path}")




